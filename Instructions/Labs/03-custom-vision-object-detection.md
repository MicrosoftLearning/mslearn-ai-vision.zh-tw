---
lab:
  title: 使用 Azure AI 自訂視覺，偵測影像中的物件
---

# 使用 Azure AI 自訂視覺，偵測影像中的物件

在此練習中，您將使用自訂視覺服務來定型「物件偵測」** 模型，以偵測並找出影像中的三種水果 (蘋果、香蕉和柳橙)。

## 建立自訂視覺資源

如果您的 Azure 訂用帳戶中已有可用來訓練、預測的**自訂視覺**資源，即可使用它們，或可在本練習裡的現有多種服物務帳戶中使用。 如果沒有，請使用下列指示來建立。

> **備註**：如果使用到多種服務資源，那麼訓練、預測的金鑰和端點就會一樣。

1. 在新的瀏覽器索引標籤中，開啟 Azure 入口網站 (位於 `https://portal.azure.com`)，使用與您的 Azure 訂用帳戶相關聯的 Microsoft 帳戶進行登入。
1. 選取 [&65291;建立資源]**** 按鈕，搜尋*自訂視覺*，然後使用下列設定建立**自訂視覺**資源：
    - **建立選項**：兩者
    - **訂用帳戶**：您的 Azure 訂用帳戶**
    - **資源群組**：*選擇或建立資源群組 (如果您使用受限制的訂用帳戶，則您可能沒有建立新資源群組的權限 - 請使用所提供的資源群組)*
    - **區域**：*選擇任何可用的區域*
    - **名稱**：輸入唯一名稱**
    - **定型定價層**：F0
    - **預測定價層**：F0

    > **注意**：若您的訂用帳戶中已經有 F0 自訂視覺服務，請為這一個選取 [S0]****。

1. 等候資源建立，然後檢視部署詳細資料，並注意已佈建兩個自訂視覺資源；一個用於定型，另一個用於預測 (以 **-Prediction** 尾碼顯示)。 您可以透過導覽到建立它們的資源群組來檢視這些資源。

> **重要**：每個資源都有自己的*端點*和*金鑰*，可用來管理來自程式碼的存取。 若要定型影像分類模型，您的程式碼必須使用*定型*資源 (搭配其端點和金鑰)；而若要使用定型的模型來預測影像類別，您的程式碼必須使用*預測*資源 (搭配其端點和金鑰)。

## 複製本課程的存放庫

您將可使用 Azure 入口網站中的 Cloud Shell，開發程式碼。 您應用程式的程式碼檔案已在 GitHub 存放庫中提供。

> **秘訣**：若您最近已複製 **mslearn-ai-vision** 存放庫，您可以跳過這項工作。 否則，請遵循下列步驟將其複製到您的開發環境。

1. 使用頁面上方搜尋欄右側的 [\>_]**** 按鈕，即可到 Azure 入口網站上，建立新的 Cloud Shell，選取 [PowerShell]****** 環境。 Cloud Shell 會在 Azure 入口網站底部的窗格顯示命令列介面。

    > **注意**：如果您之前建立了使用 *Bash* 環境的 Cloud Shell，請將其切換到 ***PowerShell***。

1. 在 Cloud Shell 工具列中，在**設定**功能表中，選擇**轉到經典版本**（這是使用程式碼編輯器所必需的）。

    > **提示**：當您將命令貼到 Cloud Shell 中時，輸出可能會佔用大量的螢幕緩衝區。 您可以透過輸入 `cls` 命令來清除螢幕，以便更輕鬆地專注於每個工作。

1. 在 PowerShell 窗格中，輸入以下命令來複製此練習的 GitHub 存放庫：

    ```
    rm -r mslearn-ai-vision -f
    git clone https://github.com/microsoftlearning/mslearn-ai-vision mslearn-ai-vision
    ```

1. 複製存放庫之後，瀏覽至包含應用程式碼檔案的資料夾：  

    ```
   cd mslearn-ai-vision/Labfiles/03-object-detection
    ```

## 建立自訂視覺專案

若要訓練物件偵測模型，您必須根據訓練資源來建立自訂視覺專案。 為此，您將使用自訂視覺入口網站。

1. 在新的瀏覽器索引標籤中，開啟位於 `https://customvision.ai` 的自訂視覺入口網站，並使用與您的 Azure 訂用帳戶相關聯的 Microsoft 帳戶登入。
1. 建立包含下列設定的新專案：
    - **名稱**：偵測水果
    - **描述**：針對水果的物件偵測。
    - **資源**：*您先前建立的自訂視覺資源*
    - **專案類型**：物件偵測
    - **網域**：一般
1. 等候專案建立並在瀏覽器中開啟。

## 新增並標記影像

若要訓練物件偵測模型，您必須上傳影像 (其包含想要模型識別的類別)，並加以標記來表示每個物件執行個體的周框方塊。

1. 從 `https://github.com/MicrosoftLearning/mslearn-ai-vision/raw/main/Labfiles/03-object-detection/training-images.zip` 下載訓練影像，並解壓縮 zip 資料夾以檢視其內容。 此資料夾包含水果的影像。
1. 在自訂視覺入口網站，您的物件偵測專案中，選取 [新增影像] **** 並上傳已擷取資料夾中的影像。
1. 影像上傳完成後，選取第一個加以開啟。
1. 將滑鼠游標暫留在影像中的任一物件上，直到如下影像顯示自動偵測的區域。 然後選取物件，如有必要，請調整區域大小以將其環繞。

    ![物件的預設區域](../media/object-region.jpg)

    或者，您可以簡單地在物件周圍拖曳以建立區域。

1. 當區域環繞物件時，新增具有適當物件類型 (「蘋果」**、「香蕉」** 或「柳橙」**) 的新標籤，如下所示：

    ![影像中一個已標記的物件](../media/object-tag.jpg)

1. 選取並標記影像中的每個其他物件，調整區域大小並視需要新增標籤。

    ![影像中兩個已標記的物件](../media/object-tags.jpg)

1. 使用右側的 **>** 連結移至下一個影像，並標記其物件。 然後，只需繼續處理整個影像集合並標記每個蘋果、香蕉與柳橙即可。

1. 當您完成最後影像標記時，請先關閉 [ **影像細節** 資料] 編輯器。 請在 [**訓練影像]** 頁面上的 [標記 **] 下方**，選取 **[** 已標記]，即可查看所有已標記的影像：

![專案中已標記的影像](../media/tagged-images.jpg)

## 使用定型 API 來上傳影像

您可以使用自訂視覺入口網站中的 UI 標記影像，但許多 AI 開發小組會使用其他工具，以便產生檔案，其中包含影像中的標記、物件區域等相關資訊。 在這類案例中，您可以使用自訂視覺定型 API 將已標記的影像上傳至專案。

> **注意**：在此練習中，您可以選擇從 **C#** 或 **Python** SDK 使用 API。 在下列步驟中，執行適合您慣用語言的動作。

1. 按一下自訂視覺入口網站中 [定型影像]**** 頁面右上方的 [設定]** (&#9881;) 圖示，以檢視專案設定。
1. 在左側的 [一般]**** 之下，記下可唯一識別此專案的 [專案識別碼]****。
1. 在右側的 [資源]**** 之下，注意已顯示金鑰和端點。 這些是*定型*資源的詳細資料 (您也可以在 Azure 入口網站中檢視資源以取得這項資訊)。
1. 返回 Azure 入口網站，根據您的使用語言執行命令 `cd C-Sharp/train-detector` 或 `cd Python/train-detector`。
1. 針對您的使用語言執行適當的命令，以安裝自訂視覺訓練套件：

    **C#**

    ```
   dotnet add package Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training --version 2.0.0
    ```

    **Python**

    ```
   pip install azure-cognitiveservices-vision-customvision==3.1.1
    ```

1. 使用 `ls` 命令，您可以檢視 **train-detector** 資料夾的內容。 您會發現其中還包含組態設定的檔案：

    - **C#**：appsettings.json
    - **Python**：.env

1. 輸入以下命令，編輯已提供的設定檔：

    **C#**

    ```
   code appsettings.json
    ```

    **Python**

    ```
   code .env
    ```

    程式碼編輯器中會開啟檔案。

1. 在程式碼檔案中，更新其中包含的設定值，以反映自訂視覺*訓練*資源的**端點**和驗證**金鑰**，以及您先前建立的物件偵測專案的專案識別碼。
1. 取代預留位置後，在程式碼編輯器中使用 **CTRL+S** 命令或**按下滑鼠右鍵 > [儲存]** 來儲存變更，然後使用 **CTRL+Q** 命令或**按下滑鼠右鍵 > [結束]** 來關閉程式碼編輯器，同時保持 Cloud Shell 命令列開啟。
1. 執行 `code tagged-images.json` 以開啟檔案並檢查其中包含的 JSON。 JSON 會定義一份影像清單，每個影像都包含一或多個已標記的區域。 每個已標記的區域都包含標記名稱，以及含有已標記物件的週框方塊的頂端和左座標及寬度和高度維度。

    > **注意**：此檔案中的座標和維度表示影像上的相對點。 例如，「高度」** 值為 0.7 表示影像高度 70% 的方塊。 有些標記工具會產生其他格式的檔案，其中座標和維度值代表像素、英吋或其他測量單位。

1. 請注意，**train-detector** 資料夾包含子資料夾，其中儲存 JSON 檔案所參考的影像檔案。

1. 請注意，**text-analysis** 資料夾包含用戶端應用程式的程式碼檔案：

    - **C#**：Program.cs
    - **Python**：train-detector.py

    開啟程式碼檔案並檢閱其所包含的程式碼，並注意下列詳細資料：
    - 從您安裝的套件匯入命名空間
    - **Main** 函式會擷取組態設定，並使用金鑰和端點來建立已驗證的 **CustomVisionTrainingClient**，然後搭配專案識別碼使用以建立專案的 **Project** 參考。
    - **Upload_Images**函式會從 JSON 檔案擷取已標記的區域資訊，並用以建立具有區域的影像批次，然後再上傳至專案。

1. 執行下列命令以執行程式：
    
    **C#**
    
    ```
   dotnet run
    ```
    
    **Python**
    
    ```
   python train-detector.py
    ```
    
1. 等候程式結束。 然後返回瀏覽器，並在自訂視覺入口網站中檢視專案的 [定型影像]**** 頁面 (視需要重新整理瀏覽器)。
1. 確認已將一些已標記的新影像新增至專案。

## 訓練並測試模型

現在您已經在專案中標記影像，可開始訓練模型。

1. 在自訂視覺專案中，按一下 [訓練]**** 以使用已標記的影像來訓練物件偵測模型。 選取 [Quick Training] (快速訓練)**** 選項。
1. 等待訓練完成 (這可能需要十分鐘左右)，然後檢閱精確度**、重新叫用** 和對應** 效能計量，這些計量測量分類模型的預測正確性，各項計量應該都較高。
1. 在頁面的右上方，按一下 [Quick Test] (快速測試)****，然後在 [影像 URL]**** 方塊中，輸入 `https://aka.ms/apple-orange` 並檢視所產生的預測。 然後關閉 [Quick Test] (快速測試)**** 視窗。

## 發佈物件偵測模型

您現在已準備好發佈已定型的模型，以便從用戶端應用程式使用該模型。

1. 在自訂視覺入口網站的 [效能]**** 頁面上，按一下 [&#128504; 發佈]**** 以發佈具有下列設定的已定型模型：
    - **模型名稱**：fruit-detector
    - **預測資源**：*您先前建立的**預測**資源，其結尾是 "-Prediction" (<u>不是</u>定型資源)*。
1. 在 [專案設定] **** 頁面的左上方，按一下*專案資源庫* (&#128065;) 圖示以返回自訂視覺入口網站首頁，現在您的專案已在其中列出。
1. 在自訂視覺入口網站首頁的右上方，按一下設定 ** (&#9881;) 圖示以檢視自訂視覺服務的設定。 然後，在 [資源]**** 之下，尋找以 "-Prediction" 結尾的*預測*資源 (<u>不是</u>定型資源) 來判斷其**金鑰**和**端點**值 (您也可在Azure 入口網站中檢視資源以取得這項資訊)。

## 從用戶端應用程式使用影像分類器

既然您已發佈影像分類模型，即可從用戶端應用程式使用。 同樣地，您可以選擇使用 **C#** 或 **Python**。

1. 在 Azure 入口網站中，瀏覽至執行 `cd ../test-detector` 命令的 **test-detector** 資料夾。
1. 輸入下列特定 SDK 命令來安裝自訂視覺預測套件：

    **C#**

    ```
   dotnet add package Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction --version 2.0.0
    ```

    **Python**

    ```
   pip install azure-cognitiveservices-vision-customvision==3.1.1
    ```

> **注意**：Python SDK 套件同時包含定型和預測套件，而且可能已經安裝。

1. 開啟用戶端應用程式的組態檔 (適用於 C# 的*appsettings.json* 或適用於 Python 的 *.env*)，並更新其中包含的組態值，以反映自訂視覺「預測」** 資源的端點和金鑰、物件偵測專案的專案識別碼，以及已發佈模型的名稱 (應該是 *fruit-detector*)。 儲存您的變更並關閉該檔案。
1. 開啟用戶端應用程式的程式碼檔案 (適用於 C# 的 *Program.cs*，適用於 Python 的 *test-detector.py*)，並檢閱其所包含的程式碼，並注意下列詳細資料：
    - 從您安裝的套件匯入命名空間
    - **Main** 函式會擷取組態設定，並使用金鑰和端點來建立已驗證的 **CustomVisionPredictionClient**。
    - 預測用戶端物件用於取得 **produce.jpg** 影像的物件偵測預測，並在要求中指定專案識別碼和模型名稱。 預測的標記區域接著會繪製在影像上，結果會儲存為 **output.jpg**。
1. 關閉程式碼編輯器，然後輸入下列命令以執行程式：

    **C#**

    ```
   dotnet run
    ```

    **Python**

    ```
   python test-detector.py
    ```

1. 程式完成後，在 Cloud Shell 工具列中，選取 [上傳/下載檔案]****，然後選取 [下載]****。 在新的對話方塊中，輸入下列檔案路徑，然後選取 [下載]****：

    **C#**
   
    ```
   mslearn-ai-vision/Labfiles/03-object-detection/C-Sharp/test-detector/output.jpg
    ```

    **Python**
   
    ```
   mslearn-ai-vision/Labfiles/03-object-detection/Python/test-detector/output.jpg
    ```

1. 檢視所產生的 **output.jpg** 檔案，以查看影像中偵測到的物件。

## 清除資源

如果您不將本實驗中所建立的 Azure 資源用於其他訓練模組，則可以將其刪除以避免產生進一步的費用。

1. 開啟 Azure 入口網站 (位於 `https://portal.azure.com`)，然後在頂端搜尋列中搜尋您在此實驗中所建立的資源。

1. 在資源頁面上，選取 [刪除]**** 並依照指示刪除該資源。 或者，您也可以刪除整個資源群組以同時清理所有資源。
   
## 其他相關資訊

如需使用自訂視覺服務進行物件偵測的詳細資訊，請參閱[自訂視覺文件](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/)。
